{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42df563c-a18f-46e4-a578-3b83ed9e697c",
   "metadata": {},
   "source": [
    "#### Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc021511-a7ec-4bcb-9ade-5f73154cd8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "import pathlib\n",
    "import shutil\n",
    "import itertools\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from PIL import Image as pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351e1291-8fac-4e58-9d80-afaa8da85099",
   "metadata": {},
   "source": [
    "Will be using the expanded dataset for potentially better generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1da31181-bc08-4f05-b855-ae23a32c25ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory2 = \"brain_tumor 2/test/\"\n",
    "pixel2 = []\n",
    "target2 = []\n",
    "\n",
    "for dataclass in glob(directory2+'/*'):\n",
    "    for file in glob(dataclass+'/*'):\n",
    "        pixels = mpl.image.imread(file)\n",
    "        pixel2.append(pixels) \n",
    "        target2.append(dataclass.split(\"/\")[-1])\n",
    "        \n",
    "directory3 = \"brain_tumor 2/training/\"\n",
    "\n",
    "for dataclass in glob(directory3+'/*'):\n",
    "    for file in glob(dataclass+'/*'):\n",
    "        pixels = mpl.image.imread(file)\n",
    "        pixel2.append(pixels) \n",
    "        target2.append(dataclass.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e325b5a6-35d4-4af6-a9f2-494d16afaae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8976"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame({'image': pixel2,'target': target2})\n",
    "df2[\"target\"] = df2.target.str.replace(\"pred\",\"test_set\")\n",
    "len(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fca16f-987d-4cdf-a020-6a9da26f613d",
   "metadata": {},
   "source": [
    "With additional set of data, 65% of the final dataset (c. 9000 images) is positive class and 35% is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b321d6e0-64ad-42bb-965e-66556c68ec3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.patches.Wedge at 0x7ff726a23700>,\n",
       "  <matplotlib.patches.Wedge at 0x7ff726a23e20>],\n",
       " [Text(-0.5139468846762569, 0.972552620546349, 'yes'),\n",
       "  Text(0.5139468846762568, -0.972552620546349, 'no')],\n",
       " [Text(-0.2803346643688674, 0.5304832475707357, '65%'),\n",
       "  Text(0.2803346643688673, -0.5304832475707357, '35%')])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pie(df2[df2.target!=\"test_set\"].target.value_counts(), labels=['yes','no'], autopct='%0.0f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10666f3e-daf4-4f2a-835d-500821679aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = pathlib.Path(\"brain_tumor 2/training\")\n",
    "test_dir = pathlib.Path(\"brain_tumor 2/validation\")\n",
    "pred_dir = pathlib.Path(\"brain_tumor 2/test\")\n",
    "\n",
    "batch = 256\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "img_size = (img_height, img_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6719b312-c5c1-4edc-952c-dd415f898c3e",
   "metadata": {},
   "source": [
    "Image augmentation applied to the training dataset for generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f5f8cbb-7f57-4fd8-a0e7-be06aede3f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagegenerator = image.ImageDataGenerator(rescale=1./255,\n",
    "                                          rotation_range=30,\n",
    "                                          width_shift_range=15,\n",
    "                                          height_shift_range=15,\n",
    "                                          brightness_range=[0.5, 1.5],\n",
    "                                          horizontal_flip=True,\n",
    "                                          vertical_flip=True)\n",
    "imggen = image.ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd718866-cc89-4bf4-9a4f-45185f1a8d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7465 images belonging to 2 classes.\n",
      "Found 1047 images belonging to 2 classes.\n",
      "Found 1511 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = imagegenerator.flow_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch,\n",
    "    target_size=img_size,\n",
    "    seed=2021,\n",
    "    color_mode='rgb',\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "test_gen = imggen.flow_from_directory(\n",
    "    test_dir,\n",
    "    batch_size=batch,\n",
    "    target_size=img_size,\n",
    "    color_mode='rgb',\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "pred_gen = imggen.flow_from_directory(\n",
    "    pred_dir,\n",
    "    batch_size=batch,\n",
    "    target_size=img_size,\n",
    "    color_mode='rgb',\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c57ba27f-a4f5-4fda-8b64-660732ada733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D, InputLayer\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4654e96b-7765-47be-b3f1-5ffcc1cb854e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 224, 224, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 112, 112, 32)      9248      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 112, 64)      18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 112, 112, 64)      102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 56, 56, 10)        31370     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 31360)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 31361     \n",
      "=================================================================\n",
      "Total params: 193,835\n",
      "Trainable params: 193,835\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-18 15:44:13.147329: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-01-18 15:44:13.148364: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model_expand = Sequential()\n",
    "\n",
    "model_expand.add(InputLayer(input_shape=(224,224,3)))\n",
    "\n",
    "model_expand.add(Conv2D(filters=32, kernel_size=3, activation='relu', padding='same'))\n",
    "model_expand.add(MaxPooling2D())\n",
    "\n",
    "model_expand.add(Conv2D(filters=32, kernel_size=3, activation='relu', padding='same'))\n",
    "\n",
    "model_expand.add(Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'))\n",
    "\n",
    "model_expand.add(Conv2D(filters=64, kernel_size=5, activation='relu', padding='same'))\n",
    "model_expand.add(MaxPooling2D())\n",
    "\n",
    "model_expand.add(Conv2D(filters=10, kernel_size=7, activation='relu', padding='same'))\n",
    "model_expand.add(Flatten())\n",
    "model_expand.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_expand.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_expand.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de243f70-3aa3-4a03-837a-f2555ab19851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "30/30 [==============================] - 1322s 44s/step - loss: 0.6234 - accuracy: 0.7328 - val_loss: 0.5471 - val_accuracy: 0.7230\n",
      "Epoch 2/100\n",
      "30/30 [==============================] - 1243s 41s/step - loss: 0.3905 - accuracy: 0.8269 - val_loss: 0.5122 - val_accuracy: 0.7775\n",
      "Epoch 3/100\n",
      "30/30 [==============================] - 1206s 40s/step - loss: 0.2930 - accuracy: 0.8770 - val_loss: 0.4838 - val_accuracy: 0.8099\n",
      "Epoch 4/100\n",
      "30/30 [==============================] - 1234s 41s/step - loss: 0.2303 - accuracy: 0.9084 - val_loss: 0.4812 - val_accuracy: 0.8500\n",
      "Epoch 5/100\n",
      "30/30 [==============================] - 1264s 42s/step - loss: 0.1912 - accuracy: 0.9283 - val_loss: 0.5968 - val_accuracy: 0.8309\n",
      "Epoch 6/100\n",
      "30/30 [==============================] - 1285s 43s/step - loss: 0.1463 - accuracy: 0.9504 - val_loss: 0.4077 - val_accuracy: 0.8720\n",
      "Epoch 7/100\n",
      "30/30 [==============================] - 1435s 48s/step - loss: 0.1114 - accuracy: 0.9633 - val_loss: 0.3554 - val_accuracy: 0.8883\n",
      "Epoch 8/100\n",
      "30/30 [==============================] - 1153s 38s/step - loss: 0.0878 - accuracy: 0.9707 - val_loss: 0.3659 - val_accuracy: 0.9064\n",
      "Epoch 9/100\n",
      "30/30 [==============================] - 1137s 38s/step - loss: 0.0721 - accuracy: 0.9771 - val_loss: 0.3650 - val_accuracy: 0.8997\n",
      "Epoch 10/100\n",
      "30/30 [==============================] - 1135s 38s/step - loss: 0.0646 - accuracy: 0.9763 - val_loss: 0.3799 - val_accuracy: 0.9121\n",
      "Epoch 11/100\n",
      "30/30 [==============================] - 1146s 38s/step - loss: 0.0456 - accuracy: 0.9854 - val_loss: 0.3683 - val_accuracy: 0.9179\n",
      "Epoch 12/100\n",
      "30/30 [==============================] - 1164s 39s/step - loss: 0.0384 - accuracy: 0.9865 - val_loss: 0.4648 - val_accuracy: 0.9140\n",
      "Epoch 13/100\n",
      "30/30 [==============================] - 1148s 38s/step - loss: 0.0192 - accuracy: 0.9944 - val_loss: 0.4493 - val_accuracy: 0.9226\n",
      "Epoch 14/100\n",
      "30/30 [==============================] - 1129s 38s/step - loss: 0.0133 - accuracy: 0.9962 - val_loss: 0.5136 - val_accuracy: 0.9217\n",
      "Epoch 15/100\n",
      "30/30 [==============================] - 1135s 38s/step - loss: 0.0110 - accuracy: 0.9969 - val_loss: 0.6021 - val_accuracy: 0.9226\n",
      "Epoch 16/100\n",
      "30/30 [==============================] - 1156s 40s/step - loss: 0.0089 - accuracy: 0.9976 - val_loss: 0.5994 - val_accuracy: 0.9245\n",
      "Epoch 17/100\n",
      "30/30 [==============================] - 1218s 41s/step - loss: 0.0084 - accuracy: 0.9983 - val_loss: 0.5944 - val_accuracy: 0.9236\n",
      "Epoch 00017: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "result_expand = model_expand.fit_generator(\n",
    "    train_gen,\n",
    "    validation_data=test_gen,\n",
    "#     steps_per_epoch=100,\n",
    "    epochs=100,\n",
    "    callbacks=[EarlyStopping(patience=10, verbose=1)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99cb4ec2-18e5-4926-a992-61e533fb5681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 22:55:01.841028: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 68s 11s/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model_expand.predict(pred_gen, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7b636a-2f3c-478b-920d-fc02c7367182",
   "metadata": {},
   "source": [
    "Classification metrics show a slight drop from near 100% to 97% in accuracy - but maybe in the range where I'm comfortable to say that the model does generalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68973890-0c79-4f9d-a2e0-32740113fee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9735274652547982\n",
      "0.9701789264413518\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix\n",
    "\n",
    "print(accuracy_score(pred_gen.classes, predictions.round(0)))\n",
    "print(recall_score(pred_gen.classes, predictions.round(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8159b1de-e8b3-4144-af09-2e620f2c285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_expand.save(\"custom_cnn_expanded_data.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77f7c796-aafd-4010-a88e-fecec805fc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 22:54:58.038125: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-12-02 22:54:58.038498: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model_expand = tf.keras.models.load_model(\"custom_cnn_expanded_data.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
